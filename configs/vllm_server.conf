# Configuration file for vLLM server job
# Override default values by uncommenting and modifying the lines below

# Model configuration
# MODEL_PATH="/scratch/kb5253/models/Qwen2.5-32B-Instruct"
# DTYPE="auto"
MAX_MODEL_LEN=131072
# GPU_MEMORY_UTILIZATION=0.9
TENSOR_PARALLEL_SIZE=2

# RoPE scaling configuration (for extended context)
ROPE_SCALING="yarn"
ROPE_SCALING_FACTOR=4

# Server configuration
# HOST="0.0.0.0"
# PORT=8000

# HuggingFace cache directory
# HF_CACHE_DIR="/scratch/kb5253/hf_cache"

# Conda environment
CONDA_ENV="/scratch/kb5253/conda_envs/vllm"

# Logging configuration
# LOG_LEVEL=1  # 0=DEBUG, 1=INFO, 2=WARN, 3=ERROR
